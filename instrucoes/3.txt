**Objetivo:** Simplificar o projeto, removendo toda a complexidade do processamento de PDFs e retornando à arquitetura estável onde a IA opera diretamente sobre o arquivo `banco_dados_servicos.txt`, garantindo a integridade dos dados e a estabilidade do sistema.

-----

#### **Tarefa 1: Limpeza do Projeto**

Para evitar qualquer confusão, vamos remover os scripts de processamento de dados que se tornaram desnecessários nesta fase.

**Ação:** Apague os seguintes arquivos e pastas do seu projeto:

  * O arquivo `knowledge_builder.py` (na raiz do projeto).
  * O arquivo `knowledge_pipeline.py` (se ainda existir).
  * O arquivo `dados/knowledge_base.csv` (se existir).
  * O arquivo `dados/especificacoes_tecnicas.csv` (se existir).
  * O arquivo `dados/cadernos_tecnicos.csv` (se existir).
  * O arquivo `dados/encargos_gerais.csv` (se existir).
  * A pasta completa `/data_extraction/`.

-----

#### **Tarefa 2: Restaurar o `finder.py` à sua Versão Estável**

Este é o passo mais crítico. Vamos instruir nosso `ServicoFinder` a voltar a ler e processar o `banco_dados_servicos.txt` diretamente, aplicando a nossa normalização de texto avançada.

**Instrução:** Substitua **completamente** o conteúdo do seu arquivo `app/finder.py` pela versão abaixo.

**Código para colocar no CANVAS (`app/finder.py` - Versão Estável):**

```python
# /app/finder.py
import pandas as pd
from sentence_transformers import SentenceTransformer, util
import torch
import os
from rank_bm25 import BM25Okapi
from app.text_utils import TextNormalizer # Importa nosso normalizador validado

class ServicoFinder:
    """
    Versão estável e robusta do Recuperador.
    Processa o banco de dados principal diretamente na inicialização.
    """
    def __init__(self, model_name='paraphrase-multilingual-mpnet-base-v2'):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.model = SentenceTransformer(model_name, device=self.device)
        self.normalizer = TextNormalizer()
        self.dataframe = None
        self.corpus_embeddings = None
        self.bm25_index = None
        print("INFO: ServicoFinder (versão estável) inicializado.")

    def _preprocess_data(self, filepath):
        """
        Lê e pré-processa o banco de dados principal de serviços.
        """
        print(f"INFO: Processando arquivo de dados principal: {filepath}")
        df = pd.read_csv(filepath, dtype={'codigo_da_composicao': str})
        
        # Renomeia para nosso padrão interno, garantindo que todas as colunas sejam lidas
        df.rename(columns={
            'codigo_da_composicao': 'codigo', 
            'descricao_completa_do_servico_prestado': 'descricao_original',
            'unidade_de_medida': 'unidade', 
            'orgao_responsavel_pela_divulgacao': 'fonte',
            'descricao_do_grupo_de_servico': 'grupo', 
            'precos_unitarios_dos_servicos': 'preco'
        }, inplace=True, errors='ignore')

        # Garante que as colunas essenciais existam
        essential_cols = ['codigo', 'descricao_original', 'unidade', 'preco', 'fonte', 'grupo']
        for col in essential_cols:
            if col not in df.columns:
                raise ValueError(f"ERRO CRÍTICO: A coluna essencial '{col}' não foi encontrada em '{filepath}'.")

        df.fillna('', inplace=True)
        
        # Aplica a normalização avançada na descrição original
        print("INFO: Aplicando normalização de texto avançada...")
        df['descricao_normalizada'] = df['descricao_original'].apply(self.normalizer.normalize)
        
        # A coluna 'descricao' que será usada para a indexação será a normalizada
        df['descricao'] = df['descricao_normalizada']
        
        print(f"INFO: Pré-processamento concluído. {len(df)} registros carregados e normalizados.")
        return df

    def load_and_index_services(self, data_filepath, force_reindex=False):
        """
        Carrega os dados, gera os índices e embeddings.
        """
        # A lógica de cache pode ser mantida para acelerar reinicializações
        cache_dir = os.path.join('dados', 'cache')
        # ... (O agente pode manter a lógica de cache aqui se desejar)

        self.dataframe = self._preprocess_data(data_filepath)
        
        corpus = self.dataframe['descricao'].tolist()
        
        print("INFO: Criando índice de palavra-chave (BM25)...")
        tokenized_corpus = [doc.split(" ") for doc in corpus]
        self.bm25_index = BM25Okapi(tokenized_corpus)
        
        print("INFO: Gerando embeddings semânticos... (Isso pode demorar na primeira vez)")
        self.corpus_embeddings = self.model.encode(corpus, convert_to_tensor=True, show_progress_bar=True, device=self.device)
        
        print("INFO: Indexação concluída.")

    # Os métodos de busca (`find_similar_semantic`, `find_similar_keyword`, `hybrid_search`)
    # permanecem exatamente os mesmos da versão anterior, pois já estão corretos e otimizados.
    # O agente deve garantir que eles estejam presentes no arquivo.
    def find_similar_semantic(self, query: str, top_k: int):
        normalized_query = self.normalizer.normalize(query)
        query_embedding = self.model.encode(normalized_query, convert_to_tensor=True, device=self.device)
        cos_scores = util.cos_sim(query_embedding, self.corpus_embeddings)[0]
        top_results = torch.topk(cos_scores, k=min(top_k, len(self.dataframe)))
        return top_results.indices.cpu().numpy(), top_results.values.cpu().numpy()

    def find_similar_keyword(self, query: str, top_k: int):
        normalized_query = self.normalizer.normalize(query)
        tokenized_query = normalized_query.split(" ")
        doc_scores = self.bm25_index.get_scores(tokenized_query)
        top_indices = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:top_k]
        return top_indices

    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.5, 
                      predicted_group: str = None, predicted_unit: str = None, 
                      group_boost: float = 1.5, unit_boost: float = 1.2):
        # ... (código completo e correto do hybrid_search)
        semantic_indices, semantic_scores = self.find_similar_semantic(query, top_k=100)
        keyword_indices = self.find_similar_keyword(query, top_k=100)
        semantic_score_map = {idx: score for idx, score in zip(semantic_indices, semantic_scores)}
        fused_scores = {}
        for rank, idx in enumerate(semantic_indices):
            fused_scores[idx] = fused_scores.get(idx, 0) + alpha * (1 / (rank + 60))
        for rank, idx in enumerate(keyword_indices):
            fused_scores[idx] = fused_scores.get(idx, 0) + (1 - alpha) * (1 / (rank + 60))

        if predicted_group or predicted_unit:
            for idx in fused_scores:
                item_group = self.dataframe.iloc[idx].get('grupo', '')
                item_unit = self.dataframe.iloc[idx].get('unidade', '')
                if predicted_group and item_group == predicted_group: fused_scores[idx] *= group_boost
                if predicted_unit and item_unit == predicted_unit: fused_scores[idx] *= unit_boost

        reranked_indices = sorted(fused_scores.keys(), key=lambda idx: fused_scores[idx], reverse=True)
        
        top_semantic_score = 0.0
        if reranked_indices:
            top_item_index = reranked_indices[0]
            top_semantic_score = float(semantic_score_map.get(top_item_index, 0.0))

        results = []
        for idx in reranked_indices[:top_k]:
            item = self.dataframe.iloc[idx]
            results.append({
                'rank': len(results) + 1,
                'score': float(fused_scores[idx]),
                'semantic_score': float(semantic_score_map.get(idx, 0.0)),
                'codigo': item.get('codigo', 'N/A'),
                'descricao': item.get('descricao_original', 'N/A'), # Retorna a descrição original para o usuário
                'preco': item.get('preco', 0.0),
                'unidade': item.get('unidade', 'N/A'),
                'fonte': item.get('fonte', 'N/A')
            })
            
        return results, top_semantic_score
```

-----

#### **Tarefa 3: Ajustar o `main.py` para Usar a Fonte de Dados Correta**

Apenas precisamos garantir que o `main.py` está a instruir o `finder` a carregar o arquivo correto.

**Trecho para Modificar em `app/main.py`:**
*Dentro da função `lifespan`, ajuste o caminho do arquivo.*

```python
# Em app/main.py, dentro da função lifespan

# ...
# Aponte o Finder para o banco de dados principal e bruto
DATA_FILE_PATH = os.path.join(os.path.dirname(__file__), '..', 'dados', 'banco_dados_servicos.txt')
# ...

# A chamada para o finder deve usar esta variável
finder_instance.load_and_index_services(data_filepath=DATA_FILE_PATH)
# ...
```

-----

#### **Tarefa 4: Re-ativar e Validar**

1.  **Limpe o Cache Antigo:** Apague a pasta `/dados/cache` para forçar a re-indexação.
2.  **Reinicie o Servidor:** `uvicorn app.main:app --reload`. Aguarde a conclusão da indexação.
3.  **Execute o Teste Qualitativo:** Rode o script `testes/qualitative_test.py`.
4.  **Verifique a Integridade:** Confirme que os resultados agora mostram preços, unidades e fontes corretos, e não mais "N/A".
5.  **Execute a Validação Quantitativa:** Se a verificação qualitativa for bem-sucedida, anote os `expected_codes` corretos na `test_suite_v3.json` e rode o `validator.py` para obtermos nossa nova linha de base de assertividade.

Ao seguir este plano, retornaremos a um estado 100% funcional e confiável. A partir daí, com os resultados da nova validação em mãos, poderemos decidir com muito mais segurança e estratégia como reintroduzir o conhecimento dos PDFs da maneira correta.